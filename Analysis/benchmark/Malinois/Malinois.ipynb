{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f1a9a6-77db-4044-816c-e699706e76ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pyBigWig\n",
    "from pyfaidx import Fasta\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b67f2dc-d374-4bb4-a9e2-56fe97a747c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ======= 数据准备 =======\n",
    "\n",
    "def one_hot_encode_seq(seq, max_len=500):\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    encoded = torch.zeros((4, max_len))\n",
    "    seq = seq.upper()\n",
    "    for i in range(min(len(seq), max_len)):\n",
    "        nt = seq[i]\n",
    "        if nt in mapping:\n",
    "            encoded[mapping[nt], i] = 1\n",
    "    return encoded\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, celltypes, vocab=None):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.celltypes = celltypes\n",
    "        \n",
    "        # 如果你没有提前定义字典，这里自动创建一个简单的碱基映射\n",
    "        if vocab is None:\n",
    "            vocab = {'A': 0, 'C': 1, 'G': 2, 'T': 3, 'N': 4}\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq   = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        ct    = self.celltypes[idx]\n",
    "        return seq, label, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bd178b-dfdb-42f7-b159-5e4f3f0876e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BassetBranched(nn.Module):\n",
    "    def __init__(self, input_len=500):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(4, 300, kernel_size=19, padding=9),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "\n",
    "            nn.Conv1d(300, 200, kernel_size=11, padding=5),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "\n",
    "            nn.Conv1d(200, 200, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "        )\n",
    "\n",
    "        conv_output_len = input_len\n",
    "        conv_output_len = conv_output_len // 3  # First pool\n",
    "        conv_output_len = conv_output_len // 4  # Second pool\n",
    "        conv_output_len = conv_output_len // 4  # Third pool\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(200 * conv_output_len, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1000, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class CrossCellLineModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,base_model,cell_type_num=32,device='cuda:1'):\n",
    "        super().__init__()\n",
    "        self.base_model = nn.ModuleList([base_model for _ in range(cell_type_num)])\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self,seq,ct):\n",
    "        y = []\n",
    "        for x,z in zip(seq,ct):\n",
    "            x = self.base_model[z](x.unsqueeze(0))\n",
    "            y.append( x )\n",
    "        y = torch.concatenate(y).flatten()\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73fdd2-794d-4b0b-8066-fa155f014b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "          learning_rate: float, num_epochs: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    训练模型的主循环。\n",
    "\n",
    "    参数:\n",
    "    model (nn.Module): 要训练的模型。\n",
    "    train_loader (DataLoader): 训练数据的 DataLoader。\n",
    "    val_loader (DataLoader): 验证数据的 DataLoader。\n",
    "    learning_rate (float): 初始学习率。\n",
    "    num_epochs (int): 训练的周期数。\n",
    "    device (torch.device): 训练设备 ('cuda' 或 'cpu')。\n",
    "    \"\"\"\n",
    "    # 1. 初始化优化器和损失函数\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # 使用CrossEntropyLoss处理one-hot标签\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    # 2. 将模型移动到目标设备\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs on device: {device}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- 训练阶段 ---\n",
    "        model.train() # 设置为训练模式\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # 使用 tqdm 封装 DataLoader 以显示进度\n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\")\n",
    "        \n",
    "        for batch_idx, (seq, target, ct) in enumerate(train_loop):\n",
    "            # 1. 数据移动到设备\n",
    "            # DNA独热编码 (B, L, 4) -> (B, 4, L)\n",
    "            seq= seq.float().permute(0, 2, 1).to(device) \n",
    "            # target已经是one-hot形式 (B, 2)，不需要转换\n",
    "            target = target.to(device)  # 保持 (B, 2) 形状\n",
    "            \n",
    "            # 2. 前向传播\n",
    "            output = model(seq,ct)\n",
    "            # 3. 计算损失 - 使用CrossEntropyLoss处理one-hot标签\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            # 4. 反向传播和优化\n",
    "            optimizer.zero_grad() # 梯度清零\n",
    "            loss.backward() # 反向传播\n",
    "            optimizer.step() # 更新参数\n",
    "\n",
    "            train_loss += loss.item() * len(seq)\n",
    "            train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # --- 验证阶段 ---\n",
    "        model.eval() # 设置为评估模式\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        with torch.no_grad(): # 禁用梯度计算\n",
    "            for seq, target, ct in val_loader:\n",
    "                seq = seq.float().permute(0, 2, 1).to(device)\n",
    "                target = target.to(device)  # 保持 (B, 2) 形状\n",
    "\n",
    "                output = model(seq, ct)\n",
    "                loss = loss_fn(output, target)\n",
    "                val_loss += loss.item() * len(seq)\n",
    "\n",
    "                # 计算准确率 - 使用argmax获取预测类别\n",
    "                # output形状 (B, 2)，取最大值的索引作为预测类别\n",
    "                predicted_classes = torch.where(output>0.5, 1, 0) \n",
    "                # target形状 (B, 2)，取最大值的索引作为真实类别\n",
    "                true_classes = target\n",
    "                correct_predictions += (predicted_classes == true_classes).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = correct_predictions / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "\n",
    "def predict(model: nn.Module, data_loader: DataLoader, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    对给定数据集进行预测。\n",
    "\n",
    "    参数:\n",
    "    model (nn.Module): 训练好的 Sei 模型。\n",
    "    data_loader (DataLoader): 待预测数据的 DataLoader。\n",
    "    device (torch.device): 预测设备 ('cuda' 或 'cpu')。\n",
    "\n",
    "    返回:\n",
    "    torch.Tensor: 模型的输出 (Sigmoid激活后的概率值)。\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval() # 设置为评估模式\n",
    "    all_predictions = []\n",
    "\n",
    "    print(\"Starting prediction...\")\n",
    "    \n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "        for SEQ, _, CT in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            # 前向传播\n",
    "            SEQ = SEQ.float().permute(0, 2, 1).to(device)\n",
    "            output = model(SEQ, CT)\n",
    "            all_predictions.append(output.cpu())\n",
    "\n",
    "    # 拼接所有批次的预测结果\n",
    "    return torch.cat(all_predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76199655-c7db-4492-b737-e4bd336a1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('/cluster2/huanglab/liquan/data/eSCREEN/Demo/data/demo_dataset.pkl','rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "trainset = data['Trainset']\n",
    "testset  = data['Testset']\n",
    "validset = data['Validset']\n",
    "\n",
    "base_to_array={'A':[1,0,0,0],'C':[0,1,0,0],'G':[0,0,1,0],'T':[0,0,0,1],'N':[0,0,0,0]}\n",
    "\n",
    "cell_type_dict = {'K562':0,'HepG2':1,'hPSC':2,'A549':3}\n",
    "\n",
    "def one_hot(seq):return np.stack( np.array( [base_to_array[a] for a in seq] ) )\n",
    "\n",
    "trainset['one hot'] = None\n",
    "for i,row in tqdm(trainset.iterrows(),total=len(trainset)):\n",
    "    trainset.at[i,'one hot'] = one_hot(row['sequence'])\n",
    "train_ohe = np.stack(trainset['one hot'])\n",
    "testset['one hot'] = None\n",
    "for i,row in tqdm(testset.iterrows(),total=len(testset)):\n",
    "    testset.at[i,'one hot'] = one_hot(row['sequence'])\n",
    "test_ohe = np.stack(testset['one hot'])\n",
    "validset['one hot'] = None\n",
    "for i,row in tqdm(validset.iterrows(),total=len(validset)):\n",
    "    validset.at[i,'one hot'] = one_hot(row['sequence'])\n",
    "valid_ohe = np.stack(validset['one hot'])\n",
    "\n",
    "X_train = torch.tensor(np.stack(trainset['one hot']))\n",
    "Y_train = torch.tensor(trainset['label'].values,dtype=torch.float)\n",
    "Z_train = torch.tensor(np.stack([cell_type_dict[ct] for ct in trainset['cell_line'].values])).int()\n",
    "\n",
    "X_test = torch.tensor(np.stack(testset['one hot']))\n",
    "Y_test = torch.tensor(testset['label'].values,dtype=torch.float)\n",
    "Z_test = torch.tensor(np.stack([cell_type_dict[ct] for ct in testset['cell_line'].values])).int()\n",
    "\n",
    "X_valid = torch.tensor(np.stack(validset['one hot']))\n",
    "Y_valid = torch.tensor(validset['label'].values,dtype=torch.float)\n",
    "Z_valid = torch.tensor(np.stack([cell_type_dict[ct] for ct in validset['cell_line'].values])).int()\n",
    "\n",
    "train_data   = SeqDataset(X_train, Y_train, Z_train)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "test_data   = SeqDataset(X_test, Y_test, Z_test)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "valid_data   = SeqDataset(X_valid, Y_valid, Z_valid)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ff356-d012-497e-9479-14124f9caf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCModel = CrossCellLineModel(BassetBranched(), cell_type_num=4, device='cuda:1')\n",
    "CCModel = CCModel.to('cuda:1')\n",
    "LR = 1e-4\n",
    "EPOCHS = 20\n",
    "train(CCModel, train_loader, test_loader, LR, EPOCHS, device='cuda:1')\n",
    "torch.save(CCModel,'Malinois.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f3247e-ee2b-4afa-95db-6d0138e61c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCModel = torch.load('Malinois.pth',weights_only=False)\n",
    "test_predictions = predict(CCModel, valid_loader, 'cuda:0')\n",
    "predicted_classes = torch.where(test_predictions>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85af238-2dd3-4fd1-81a6-44ed5e26f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions = valid_predictions;labels = Y_valid\n",
    "predicted_labels = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "\n",
    "# 计算指标\n",
    "auc_val = auc(roc_curve(labels, predictions)[0],roc_curve(labels, predictions)[1])\n",
    "mcc = matthews_corrcoef(labels, predicted_labels)\n",
    "f1 = f1_score(labels, predicted_labels)\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "recall = recall_score(labels, predicted_labels)\n",
    "precision = precision_score(labels, predicted_labels)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"AUC: {auc_val:.4f}\")\n",
    "print(f\"MCC: {mcc:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.bar([1,2,3,4,5,6],[auc_val,mcc,f1,accuracy,recall,precision])\n",
    "plt.ylim([0,1])\n",
    "plt.xticks([1,2,3,4,5,6],[\"AUC\",\"MCC\",\"F1 Score\",\"Accuracy\",\"Recall\",\"Precision\"],rotation=-30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eScreen",
   "language": "python",
   "name": "escreen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
